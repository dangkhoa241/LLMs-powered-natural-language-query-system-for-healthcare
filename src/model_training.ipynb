{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d41c53d-93f8-4db5-b499-cd085185938a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dangk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:14: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.3)\n",
      "  from scipy.sparse import csr_matrix, issparse\n",
      "C:\\Users\\dangk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dangk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Label mapping:\n",
      "  0: aggregate\n",
      "  1: compare\n",
      "  2: count\n",
      "  3: filter\n",
      "  4: trend\n",
      "Train size: 600, Val size: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\dangk\\AppData\\Local\\Temp\\ipykernel_16980\\1094301799.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([1.0143, 1.0592, 1.0318, 0.9137, 0.9811])\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dangk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 16:40, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.385400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.284900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.269800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.313600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.285400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.236300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.241100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.235900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.189300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.214100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dangk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  total_flos               =   147028GF\n",
      "  train_loss               =     0.3362\n",
      "  train_runtime            = 0:16:43.19\n",
      "  train_samples            =        600\n",
      "  train_samples_per_second =      4.785\n",
      "  train_steps_per_second   =      0.598\n",
      "Evaluating on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dangk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =     0.8733\n",
      "  eval_f1                 =     0.8732\n",
      "  eval_loss               =     0.3839\n",
      "  eval_precision          =     0.8768\n",
      "  eval_recall             =     0.8733\n",
      "  eval_runtime            = 0:00:29.00\n",
      "  eval_samples            =        600\n",
      "  eval_samples_per_second =     20.686\n",
      "  eval_steps_per_second   =      2.586\n",
      "Saving best model and tokenizer to C:\\Users\\dangk\\OneDrive\\Desktop\\Fall 2025\\ADA\\Project\\LLMs-powered-natural-language-query-system-for-healthcare\\intent_model\n",
      "Done. This model can now be loaded in app.py using model='intent_model'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "# 1. Paths & config\n",
    "ROOT_DIR = Path.cwd().parent\n",
    "\n",
    "DATA_PATH = ROOT_DIR / \"data\" / \"intent_dataset.csv\"\n",
    "MODEL_DIR = ROOT_DIR / \"intent_model\"\n",
    "LOG_DIR  = ROOT_DIR / \"logs\"\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "MAX_LENGTH = 64\n",
    "TEST_SIZE = 0.50\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# 2. Reproducibility\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "# 3. Load dataset\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Try to find text column\n",
    "TEXT_COL_CANDIDATES = [\"text\", \"query\", \"question\", \"utterance\"]\n",
    "LABEL_COL_CANDIDATES = [\"label\", \"intent\", \"target\"]\n",
    "\n",
    "text_col = next((c for c in TEXT_COL_CANDIDATES if c in df.columns), None)\n",
    "label_col = next((c for c in LABEL_COL_CANDIDATES if c in df.columns), None)\n",
    "\n",
    "if text_col is None:\n",
    "    raise ValueError(\n",
    "        f\"Could not find a text column. Tried: {TEXT_COL_CANDIDATES}. \"\n",
    "        f\"Columns in CSV: {list(df.columns)}\"\n",
    "    )\n",
    "\n",
    "if label_col is None:\n",
    "    raise ValueError(\n",
    "        f\"Could not find a label/intent column. Tried: {LABEL_COL_CANDIDATES}. \"\n",
    "        f\"Columns in CSV: {list(df.columns)}\"\n",
    "    )\n",
    "\n",
    "df = df[[text_col, label_col]].dropna().reset_index(drop=True)\n",
    "\n",
    "# Normalise labels to str (e.g. \"filter\", \"aggregate\", ...)\n",
    "df[label_col] = df[label_col].astype(str)\n",
    "\n",
    "labels = sorted(df[label_col].unique())\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "df[\"label_id\"] = df[label_col].map(label2id)\n",
    "\n",
    "print(\"Label mapping:\")\n",
    "for l, i in label2id.items():\n",
    "    print(f\"  {i}: {l}\")\n",
    "\n",
    "# 4. Train / validation split\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=df[\"label_id\"],\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Val size: {len(val_df)}\")\n",
    "\n",
    "# 5. Tokenizer & dataset class\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class IntentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=64):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = list(labels)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_ds = IntentDataset(\n",
    "    train_df[text_col].tolist(),\n",
    "    train_df[\"label_id\"].tolist(),\n",
    "    tokenizer,\n",
    "    max_len=MAX_LENGTH,\n",
    ")\n",
    "\n",
    "val_ds = IntentDataset(\n",
    "    val_df[text_col].tolist(),\n",
    "    val_df[\"label_id\"].tolist(),\n",
    "    tokenizer,\n",
    "    max_len=MAX_LENGTH,\n",
    ")\n",
    "\n",
    "# 6. Model with class weights\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# Class weights: inverse frequency\n",
    "label_counts = train_df[\"label_id\"].value_counts().sort_index()\n",
    "class_weights = (1.0 / label_counts).values\n",
    "class_weights = class_weights / class_weights.sum() * len(class_weights)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "print(\"Class weights:\", class_weights_tensor)\n",
    "\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "        if self.class_weights is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "        loss = loss_fct(logits, labels)\n",
    "    \n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "\n",
    "\n",
    "# 7. Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels_ids = eval_pred\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "\n",
    "    acc = accuracy_score(labels_ids, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels_ids, preds, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "# 8. TrainingArguments (logs + early stopping)\n",
    "#    NOTE: requires transformers >= 4.x\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(MODEL_DIR),\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=str(LOG_DIR),\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=2,  # stop if no improvement for 2 evals\n",
    "    early_stopping_threshold=0.0,\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    class_weights=class_weights_tensor,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 9. Train\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "trainer.save_state()\n",
    "\n",
    "metrics = train_result.metrics\n",
    "metrics[\"train_samples\"] = len(train_ds)\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "\n",
    "print(\"Evaluating on validation set...\")\n",
    "eval_metrics = trainer.evaluate(eval_dataset=val_ds)\n",
    "eval_metrics[\"eval_samples\"] = len(val_ds)\n",
    "trainer.log_metrics(\"eval\", eval_metrics)\n",
    "trainer.save_metrics(\"eval\", eval_metrics)\n",
    "\n",
    "# 10. Save best model + tokenizer\n",
    "print(f\"Saving best model and tokenizer to {MODEL_DIR}\")\n",
    "trainer.save_model(str(MODEL_DIR))\n",
    "tokenizer.save_pretrained(str(MODEL_DIR))\n",
    "\n",
    "print(\"Done. This model can now be loaded in app.py using model='intent_model'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e01fb92d-4683-4012-a3f5-52798f2c5d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'count', 'score': 0.9980754852294922}]\n",
      "[{'label': 'aggregate', 'score': 0.9981518387794495}]\n",
      "[{'label': 'filter', 'score': 0.9985373020172119}]\n",
      "[{'label': 'compare', 'score': 0.9982607960700989}]\n",
      "[{'label': 'filter', 'score': 0.9980431795120239}]\n"
     ]
    }
   ],
   "source": [
    "# Test after trained\n",
    "from transformers import pipeline\n",
    "\n",
    "clf = pipeline(\"text-classification\", model=\"../intent_model\", tokenizer=\"../intent_model\")\n",
    "\n",
    "print(clf(\"How many patients were admitted last year?\"))\n",
    "print(clf(\"Show average billing by insurance provider\"))\n",
    "print(clf(\"List diabetic patients\"))\n",
    "print(clf(\"Compare male and female patients\"))\n",
    "print(clf(\"Show patients over age 60\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e7701-8561-4ab3-8db9-e7395563d460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
